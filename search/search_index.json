{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"Exploratory%20Data%20Analysis/","title":"Exploratory Data Analysis","text":""},{"location":"Exploratory%20Data%20Analysis/#purpose","title":"Purpose","text":"<ul> <li>Used to identify patterns which can be observed by naked eye</li> </ul>"},{"location":"Exploratory%20Data%20Analysis/#types-of-analysis","title":"Types of analysis","text":"<ul> <li>Univariate - raw data info</li> <li>Bivariate - link between label and dependent variable</li> <li>Mulivariate - link between mltiple variables - genrally dimensionality reduction is used and visualised in 2D or 3D</li> </ul>"},{"location":"Exploratory%20Data%20Analysis/#examples-of-analysis-from-the-notebooks-in-the-site","title":"Examples of analysis from the notebooks in the site","text":"<ul> <li>Univariate - PIMA data (logistic regression): We plot each variable of the data and observe right skewed data. The outliers drag the regression line far so we clean up the data to reduce outliers.</li> <li>Bivariate - linear regression data: We plot relationship between income, age and experience and observe no clear reationship between age and income. So we go ahead and work with experience and income columns.</li> </ul>"},{"location":"Exploratory%20Data%20Analysis/#will-keep-on-updating-as-we-proceed","title":"Will keep on updating as we proceed","text":""},{"location":"Exploratory%20Data%20Analysis/#univariate-analysis","title":"Univariate analysis","text":""},{"location":"Exploratory%20Data%20Analysis/#numerical-data","title":"Numerical Data","text":"<ul> <li>Histograms: Observe data skew and outliers</li> <li>Distplot: Histogram with a probability distribution curve</li> <li>Boxplot: Used to identify mean, median. Can also help in identifying skew</li> </ul>"},{"location":"Exploratory%20Data%20Analysis/#categorical-data","title":"Categorical Data","text":"<ul> <li>Count Plot: Count the values belonging to each category</li> <li>Pie Chart: Helps visually identify percentage of categorical data</li> </ul>"},{"location":"Exploratory%20Data%20Analysis/#multivariate-analysis","title":"Multivariate Analysis","text":"<ul> <li>Scatter Plot: Identify relationships</li> <li>Bar plot: Identify data rellationships</li> </ul>"},{"location":"linearRegression/","title":"Linear Regression","text":""},{"location":"linearRegression/#purpose","title":"Purpose","text":"<ul> <li>Helps find the relationship between features and label</li> <li>Generally 2 dim, but can be scaled to any dimension</li> <li>Any statistical model which can fit a linear equation can be approximately classified by linear regression</li> </ul>"},{"location":"linearRegression/#general-use-cases","title":"General use cases","text":"<ul> <li>Real estate: Predicting house prices based on square footage, number of bedrooms, and location. </li> <li>Marketing: Estimating sales based on advertising budget. </li> <li>Education: Analyzing the relationship between study hours and test scores. </li> <li>Healthcare: Predicting medical costs based on patient factors like age and health conditions. </li> <li>Agriculture: Estimating crop yield based on fertilizer and water levels</li> </ul>"},{"location":"linearRegression/#implementation","title":"Implementation","text":"<p><pre><code>import kagglehub\n\n# Download latest version from kaggle\npath = kagglehub.dataset_download(\"hussainnasirkhan/multiple-linear-regression-dataset\")\nprint(\"Path to dataset files:\", path)\n\n# Read csv\ndf = pd.read_csv(f\"{path}\" + \"/multiple_linear_regression_dataset.csv\", sep=\",\")\n\n#EDA\nimport matplotlib.pyplot as plt\nplt.subplot(2,1,1)\nplt.plot(df[\"experience\"], df[\"income\"], 'ro')\nplt.subplot(2,1,2)\nplt.plot(df[\"age\"], df[\"income\"], 'ro')\nplt.show()\n</code></pre> </p> <p>From the image we can observe that income is dependent on experience.</p> PythonsklearnPytorch <p><pre><code>import numpy as np\nclass linearRegressor:\n    def __init__(self):\n        self.w = 0.3 \n        self.b = 0.4\n\n    def forward(self, X):\n        y_pred = [self.w*x + self.b for x in X]\n        return y_pred\n\n    def loss(self, y_pred, y_expected):\n        y_pred = np.array(y_pred)\n        y_expected = np.array(y_expected)\n        return np.mean((y_expected-y_pred)**2)\n\n    def backward(self, y, X, lr = 0.01):\n        f = y - (self.w*X + self.b)\n        N = len(X)\n        self.w -= lr * (-2 * X.dot(f).sum() / N)\n        self.b -= lr * (-2 * f.sum() / N)\n        return self.w, self.b\n\nlr = linearRegressor()\nlosses = []\ny_train  = df[\"income\"]\nX_train  = np.array(df[\"experience\"])\ny_pred = lr.forward(df[\"experience\"])\nlr.loss(y_pred, df[\"income\"])\nw,b = lr.backward(df[\"income\"], df[\"experience\"])\nfor i in range(200):\n    y_pred = lr.forward(X_train)\n    loss = lr.loss(y_pred, y_train)\n    lr.backward(y_train, X_train, lr=0.01)\n    losses.append(loss)\n# Plot the learning process\nplt.plot(range(len(losses)), losses)\nplt.xlabel('Iteration')\nplt.ylabel('Loss')\nplt.show()\n</code></pre> </p> <p><pre><code>plt.plot(df[\"experience\"], df[\"income\"], 'ro')\nplt.plot(X_train, y_pred, color = \"g\")\nplt.show()\n</code></pre> </p> <p><pre><code>from sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\nmodel = LinearRegression()\nX_train = np.array(X_train).reshape(-1,1)\ny_train = np.array(y_train).reshape(-1,1)\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_train)\nprint(\"Coefficients: \\n\", model.coef_, model.intercept_)\n# The mean squared error\nprint(\"Mean squared error: %.2f\" % mean_squared_error(y_train, y_pred))\n# The coefficient of determination: 1 is perfect prediction\nprint(\"Coefficient of determination: %.2f\" % r2_score(y_train, y_pred))\n\n# Plot outputs\nplt.scatter(X_train, y_train, color=\"black\")\nplt.plot(X_train, y_pred, color=\"blue\", linewidth=3)\nplt.show()\n</code></pre> </p> <p><pre><code>import torch\nimport torch.nn as nn\nclass linearRegression(nn.Module):\n    def __init__(self, input_dim, output_dim):\n        super(linearRegression, self).__init__()\n        self.linear = nn.Linear(input_dim, output_dim)\n\n    def forward(self, x):\n        out = self.linear(x)\n        return out\n\nmodel = linearRegression(1, 1)\n\ncriterion = nn.MSELoss()\nlr = 0.005\noptimizer = torch.optim.SGD(model.parameters(), lr=lr)\nfor epoch in range(100):\n    inputs = torch.Tensor(X_train).requires_grad_()\n    labels = torch.Tensor(y_train)\n\n    optimizer.zero_grad()\n    outputs = model(inputs)\n    loss = criterion(outputs, labels)\n    loss.backward()\n    optimizer.step()\n    print('epoch {}, loss {}'.format(epoch, loss.item()))\n\npredicted = model(torch.Tensor(X_train).requires_grad_()).data.numpy()\nplt.plot(X_train, y_train, 'go', label='True data', alpha=0.5)\nplt.plot(X_train, predicted, '--', label='Predictions', alpha=0.5)\nplt.legend(loc='best')\nplt.show()\n</code></pre> </p>"},{"location":"linearRegression/#when-does-linear-regression-fail","title":"When does linear regression fail?","text":"<ul> <li>Outliers: Extreme data points that can significantly skew the regression line. </li> <li>Non-linearity: When the relationship between variables isn't linear</li> <li>Collinearity: When independent variables are highly correlated with each other, making it difficult to isolate their individual effects. </li> <li>Heteroscedasticity: When the variance of the error terms is not constant across the data range.</li> </ul>"},{"location":"logisticRegression/","title":"Logistic Regression","text":""},{"location":"logisticRegression/#purpose","title":"Purpose","text":"<ul> <li>Helps in classifying a label</li> <li>I think of it as a linear classifier which draws a regression line and classifies the point as above or below the line</li> <li>Gives you a probibility of belonging to ones class</li> </ul>"},{"location":"logisticRegression/#general-use-cases","title":"General use cases","text":"<ul> <li>Email spam detection: Classify emails as spam or not spam </li> <li>Medical diagnosis: Predict medical conditions based on patient data </li> <li>Fraud detection: Identify data anomalies that indicate fraud </li> <li>Insurance policy approval: Decide whether to approve a new policy based on a driver's history and credit history </li> </ul>"},{"location":"logisticRegression/#implementation","title":"Implementation","text":""},{"location":"logisticRegression/#we-are-not-selecting-the-skinthickness-as-it-is-not-required-as-per-domain-knowledge-will-write-on-feature-selection-in-a-separate-page","title":"We are not selecting the skinthickness as it is not required as per domain knowledge. Will write on feature selection in a separate page","text":"<p><pre><code>import kagglehub\n\npath = kagglehub.dataset_download(\"uciml/pima-indians-diabetes-database\")\nprint(\"Path to dataset files:\", path)\n\n# Read csv and update the column names\nimport pandas as pd\n\ncol_names = ['pregnant', 'glucose', 'bp', 'skin', 'insulin', 'bmi', 'pedigree', 'age', 'label'] \npima = pd.read_csv(f\"{path}\" + \"/diabetes.csv\", sep=\",\", names=col_names) # Replcing names to make it easier to work with\n\n#EDA\nimport matplotlib.pyplot as plt\nplt.subplot(2,1,1)\nplt.plot(df[\"experience\"], df[\"income\"], 'ro')\nplt.subplot(2,1,2)\nplt.plot(df[\"age\"], df[\"income\"], 'ro')\nplt.show()\n</code></pre> </p>"},{"location":"logisticRegression/#what-can-we-observe-from-the-graph","title":"What can we observe from the graph","text":"<ul> <li>Most people are at the low insulin levels.</li> <li>Number of pregnencies are &lt;= 2 for more than 50% of people.</li> <li>The Major age group of the dataset is between 0 - 35.</li> <li>We can observe that the skin thickness can be ignored as most of the people have a similar skin thickness</li> </ul> Pythonscikit-learnPytorch <pre><code>class LogisticRegression:\n    def __init__(self):\n        self.w = np.random.randn(len(feature_cols)) * 0.01\n        self.b = 0.1\n\n    def _sigmod(self, z):\n        return 1/(1 + np.exp(-z))\n\n    def forward(self, X):\n        y_pred = np.dot(X, self.w) + self.b\n        y_pred = self._sigmod(y_pred)\n        return y_pred\n\n    def loss(self, y_pred, y_expected):\n        epsilon = 1e-9 # used to avoid log 0 cases\n        y1 = y_expected * np.log(y_pred + epsilon)\n        y2 = (1-y_expected)* np.log(1-y_pred + epsilon)\n        return -np.mean(y1+y2)\n\n    def backward(self, X, y_pred, y, lr = 0.0001):\n        dz = y_pred - y\n        dw = (1/len(X))*np.dot(X.T,dz)\n        db = (1/len(X)) * np.sum(dz)\n        self.w -= lr * dw\n        self.b -= lr * db\n        return \n\n    def predict(self, X):\n        threshold = .5\n        y_pred = self.forward(X)\n        y_pred = [0 if i &lt; threshold else 1 for i in y_pred]\n        return y_pred\n</code></pre> <pre><code>from sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\ny_pred = logreg.predict(X_test)\n</code></pre> <pre><code>import torch\nimport torch.nn as nn\nclass logisticRegression(nn.Module):\n    def __init__(self, input_dim, output_dim):\n        super(logisticRegression, self).__init__()\n        self.linear = nn.Linear(input_dim, output_dim)\n\n    def forward(self, x):\n        out = torch.sigmoid(self.linear(x))\n        return out\n\nmodel = logisticRegression(7,1)\ncriterion = nn.BCELoss()\noptimizer = torch.optim.SGD(model.parameters(), lr = 0.0005)\nfor epoch in range(100):\n    inputs = torch.Tensor(X_train.values).requires_grad_()\n    labels = torch.tensor(y_train.values, dtype=torch.float32)\n\n    optimizer.zero_grad()\n    outputs = model(inputs).squeeze()\n    loss = criterion(outputs, labels)\n    loss.backward()\n    optimizer.step()\n    if epoch%10 == 0:\n        print('epoch {}, loss {}'.format(epoch, loss.item()))\n</code></pre>"},{"location":"logisticRegression/#predictions","title":"Predictions","text":"<pre><code>predicted = model(torch.Tensor(X_test.values).requires_grad_()).data.numpy().flatten()\npredicted_labels = (predicted &gt; 0.5).astype(int)\ncorrect_predictions = (predicted_labels == y_test.values).sum()\nincorrect_predictions = (predicted_labels != y_test.values).sum()\n\nlabels = ['Correct', 'Incorrect']\nvalues = [correct_predictions, incorrect_predictions]\n\nplt.bar(labels, values, color=['green', 'red'])\nplt.xlabel('Prediction Type')\nplt.ylabel('Number of Predictions')\nplt.title('Number of Correct and Incorrect Predictions')\nplt.show()\n</code></pre>"}]}